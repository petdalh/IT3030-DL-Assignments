globals:
  learning_rate: 0.08
  epochs: 50
  batch_size: 12
  loss_function: cross_entropy

layers:
  - input_size: 1600
    output_size: 256
    activation: relu
    regularizer:
      type: L1
      rate: 0.001
  - input_size: 256
    output_size: 128
    activation: relu
    regularizer:
      type: L1
      rate: 0.001
  - input_size: 128
    output_size: 64
    activation: relu
    regularizer:
      type: L1
      rate: 0.001
  - input_size: 64
    output_size: 4
    activation: softmax
    regularizer:
      type: L1
      rate: 0.001
